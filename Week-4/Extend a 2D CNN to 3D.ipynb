{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01dcd9fc-1e92-47ff-b4e4-5b2a896520c1",
   "metadata": {},
   "source": [
    "# Part 1: Extend a 2D CNN to 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c19c0f-7384-4bff-83d1-1e485a7590d0",
   "metadata": {},
   "source": [
    "Last week, we trained a 2D convolutional neural network (CNN) on slices of a subset of the ACDC dataset.\n",
    "\n",
    "This week, we will train a **3D** CNN on the **entire** (3D) images of the **whole** ACDC dataset.\n",
    "\n",
    "\n",
    "You can reuse and modify the code from last week to solve this week's task. Specifically, you'll need to:\n",
    "1. [**Data**]. Make a train-validation-test split that uses all the training set.\n",
    "2. [**Model**]. Extend the model to 3D (*hint: read MONAI's documentation if needed*).\n",
    "3. [**Training**]. Since we'll train on the whole 3D images, you don't need to extract slices from the images.\n",
    "4. [**Inference**]. *Hint: Remember that now, the model produces 3D images directly*\n",
    "5. [**Metrics**]. In addition to Dice coefficient, use other metrics that you think might be relevant now. You can use the metrics from MONAI or from medpy.metrics (the latter is probably easier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "22572944-ca84-4fb0-85e1-022414c63548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import monai\n",
    "import numpy as np\n",
    "import torch\n",
    "from monai.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "from monai.transforms.utils import allow_missing_keys_mode\n",
    "from monai.transforms import BatchInverseTransform\n",
    "from monai.networks.nets import DynUNet\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "import medpy.metric as metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93559408-d4a7-4904-9826-48663c9078a6",
   "metadata": {},
   "source": [
    "## 1.1 Data: Train-validation-test splits\n",
    "\n",
    "> ---\n",
    "> **Task:** Make a train-validation-test split (70-10-20) that uses **all** the training set (100 patients).\n",
    ">\n",
    "> **Important remark**: the parameters `first_id` and `last_id` of `getPatientData` function look at the patients id, which, as you can see in the `DATA_PATH` folder, starts with \"patient001\". In other words, if you want to create a dataset with the images from the 100 patients, you need to call the function in the following way: `getPatientData(dataset_path=..., first_id=1, last_id=100)`.\n",
    "> \n",
    "> ---\n",
    "> \n"
   ]
  },
  {
   "cell_type": "code",
   "id": "735b6f95-a93e-4bc0-bcce-2e7b0e9c5a12",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "def get_patient_data(dataset_path: Path, first_id: int, last_id: int):\n",
    "    dataset = []\n",
    "    for i in range(first_id, last_id+1):\n",
    "        patient_id = f'patient{str(i).zfill(3)}'\n",
    "        tmp_path = dataset_path / patient_id\n",
    "        for path_frame in tmp_path.glob(\"*frame[0-9][0-9].nii.gz\"):\n",
    "            dataset.append({'image': str(path_frame),\n",
    "                            'label': str(path_frame).replace(\".nii.gz\", \"_gt.nii.gz\"),\n",
    "                            'id': path_frame.name.split(\".\")[0]})\n",
    "    return dataset\n",
    "\n",
    "DATA_PATH = \"data/ACDC17\"\n",
    "\n",
    "# 1. Data. Make a 70-10-20% train-validation-test split here\n",
    "train_files = get_patient_data(dataset_path=Path(DATA_PATH),\n",
    "                            first_id=1, last_id=70)\n",
    "val_files = get_patient_data(dataset_path=Path(DATA_PATH),\n",
    "                            first_id=71, last_id=80)  \n",
    "test_files = get_patient_data(dataset_path=Path(DATA_PATH),\n",
    "                            first_id=81, last_id=100)\n",
    "train_files[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "564e1036-c622-440e-aaed-efcf7ce613ef",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Verifying that your train-validation-test split is using the whole dataset\n",
    "messages = []\n",
    "if len(train_files) == 70*2:\n",
    "    messages.append(\"✅ Training set size\")\n",
    "else:\n",
    "    messages.append(\"❌ The training set must consist of the images of 70 patients (140 images in total)\")\n",
    "    \n",
    "if len(val_files) == 10*2:\n",
    "    messages.append(\"✅ Validation set size\")\n",
    "else:\n",
    "    messages.append(\"❌ The validation set must consist of the images of 10 patients (20 images in total)\")\n",
    "    \n",
    "if len(test_files) == 20*2:\n",
    "    messages.append(\"✅ Test set size\")\n",
    "else:\n",
    "    messages.append(\"❌ The test set must consist of the images of 20 patients (40 images in total)\")\n",
    "\n",
    "allimages = set()\n",
    "for im in train_files + val_files + test_files:\n",
    "    allimages.add(im['image'])\n",
    "if len(allimages) != 100*2:\n",
    "    messages.append(\"❌ There should be 200 *unique* images in the train, validation and test sets\")\n",
    "\n",
    "for m in messages:\n",
    "    print(m)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "58688d8a-90c4-4d0c-9724-ac4c047a3966",
   "metadata": {},
   "source": [
    "## 1.2 Model\n",
    "\n",
    "> ---\n",
    "> **Task:** Extend last week's model to 3D. Check MONAI's documentation for this: https://docs.monai.io/en/stable/networks.html#nets\n",
    "> \n",
    "> ---\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fd24fedd-f4dd-4ffd-a876-136f333a0b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Model. Extend the model to 3D \n",
    "model = DynUNet(\n",
    "    spatial_dims = 3,\n",
    "    in_channels = 1, \n",
    "    out_channels = 4,\n",
    "    kernel_size = [3, 3, 3, 3, 3, 3],\n",
    "    strides = [1, 2, 2, 2, 2, 2], \n",
    "    upsample_kernel_size = [2, 2, 2, 2, 2]\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0322fcb8-bec5-4c0d-ab60-48d621302827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Your model seem to be 3D\n"
     ]
    }
   ],
   "source": [
    "# Verifying that your model is 3D\n",
    "err = False\n",
    "for name, W in model.named_parameters():\n",
    "    if 'conv.weight' in name:\n",
    "        if len(W.shape) != 5:\n",
    "            err = True\n",
    "            break\n",
    "if err:\n",
    "    print(\"❌ Your model doesn't seem to be 3D\")\n",
    "else:\n",
    "    print(\"✅ Your model seem to be 3D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1654191f-ad16-4f84-926b-e72d947029e5",
   "metadata": {},
   "source": [
    "## 1.3 Training\n",
    "> ---\n",
    "> **Task:** Gather the relevant preprocessing and data-augmentation transforms for training your 3D CNN model on 3D images.\n",
    "> \n",
    "> ---\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d87c8be2-1d02-47f8-a495-beeaa5d06861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smallestVoxelDims [0.703125 0.703125 5.      ]\n",
      "largestVoxelDims [ 1.91964  1.91964 10.     ]\n",
      "smallestImDims [154 154   6]\n",
      "largestImDims [428 512  18]\n"
     ]
    }
   ],
   "source": [
    "allFiles = train_files + val_files + test_files\n",
    "\n",
    "# 1) Create an empty list that will contain the voxel resolutions of all the images\n",
    "\n",
    "# 2) Iterate over all paths in trainFiles, valFiles and testFiles\n",
    "# 2.1) Open the images, gather their voxel size/resolution, put it in the list\n",
    "voxelDims = [nib.load(file['image']).header.get_zooms() for file in allFiles]\n",
    "voxelDims = np.array(voxelDims)\n",
    "\n",
    "# 3) Find the maximum and minimum voxel resolution across all images in each dimension\n",
    "#    (i.e., maximum and minimum voxel size in the first dimension, second dimension,\n",
    "#     and third dimension, 6 numbers in total)\n",
    "smallestVoxelDims = np.min(voxelDims, axis=0)\n",
    "largestVoxelDims = np.max(voxelDims, axis=0)\n",
    "\n",
    "# Follow the same steps for the image size\n",
    "imageDims = [nib.load(file['image']).header.get_data_shape() for file in allFiles]\n",
    "imageDims = np.array(imageDims)\n",
    "\n",
    "# 3) Find the maximum and minimum voxel resolution across all images in each dimension\n",
    "#    (i.e., maximum and minimum voxel size in the first dimension, second dimension,\n",
    "#     and third dimension, 6 numbers in total)\n",
    "smallestImDims = np.min(imageDims, axis=0)\n",
    "largestImDims = np.max(imageDims, axis=0)\n",
    "\n",
    "print(\"smallestVoxelDims\", smallestVoxelDims)\n",
    "print(\"largestVoxelDims\", largestVoxelDims)\n",
    "print(\"smallestImDims\", smallestImDims)\n",
    "print(\"largestImDims\", largestImDims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "88e6fb41-5104-469c-9156-e19698b9bb43",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Compose' object has no attribute 'cuda'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[67], line 26\u001B[0m\n\u001B[1;32m     10\u001B[0m newVoxelDims \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmedian(voxelDims, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     11\u001B[0m spatialSize \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m320\u001B[39m, \u001B[38;5;241m320\u001B[39m, \u001B[38;5;241m32\u001B[39m\n\u001B[1;32m     13\u001B[0m train_transforms \u001B[38;5;241m=\u001B[39m \u001B[43mmonai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransforms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompose\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmonai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransforms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLoadImaged\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlabel\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmonai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransforms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEnsureChannelFirstd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlabel\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmonai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransforms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSpacingd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlabel\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpixdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnewVoxelDims\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbilinear\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnearest\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmonai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransforms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mResizeWithPadOrCropd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlabel\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mspatial_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mspatialSize\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#monai.transforms.RandSpatialCropd(keys=['image', 'label'], roi_size=[-1, -1, 1]),\u001B[39;49;00m\n\u001B[1;32m     19\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#monai.transforms.SqueezeDimd(keys=['image', 'label'], dim=-1),\u001B[39;49;00m\n\u001B[1;32m     20\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m## TODO: Add more Data Augmentation functions here\u001B[39;49;00m\n\u001B[1;32m     21\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#monai.transforms.RandRotated(keys=['image', 'label'], range_z = 180, prob = 0.5, mode='nearest'),\u001B[39;49;00m\n\u001B[1;32m     22\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#monai.transforms.RandGaussianSmoothd(keys=['image'], sigma_x=(0.25, 1.5), sigma_y=(0.25, 1.5), sigma_z=(0.25, 1.5), approx='erf', prob=0.5),\u001B[39;49;00m\n\u001B[1;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m#monai.transforms.RandGaussianNoised(keys=['image'], prob=0.5, mean=0.0, std=0.5),\u001B[39;49;00m\n\u001B[1;32m     24\u001B[0m \n\u001B[1;32m     25\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmonai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransforms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAsDiscreted\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlabel\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mto_onehot\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# Convert \"label\" to onehot encoded.\u001B[39;49;00m\n\u001B[0;32m---> 26\u001B[0m \u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m()\n\u001B[1;32m     28\u001B[0m val_transforms \u001B[38;5;241m=\u001B[39m monai\u001B[38;5;241m.\u001B[39mtransforms\u001B[38;5;241m.\u001B[39mCompose([\n\u001B[1;32m     29\u001B[0m     monai\u001B[38;5;241m.\u001B[39mtransforms\u001B[38;5;241m.\u001B[39mLoadImaged(keys\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m]),\n\u001B[1;32m     30\u001B[0m     monai\u001B[38;5;241m.\u001B[39mtransforms\u001B[38;5;241m.\u001B[39mEnsureChannelFirstd(keys\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m]),\n\u001B[1;32m     31\u001B[0m     monai\u001B[38;5;241m.\u001B[39mtransforms\u001B[38;5;241m.\u001B[39mSpacingd(keys\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m], pixdim\u001B[38;5;241m=\u001B[39mnewVoxelDims, mode\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbilinear\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnearest\u001B[39m\u001B[38;5;124m\"\u001B[39m]),\n\u001B[1;32m     32\u001B[0m     monai\u001B[38;5;241m.\u001B[39mtransforms\u001B[38;5;241m.\u001B[39mResizeWithPadOrCropd(keys\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m], spatial_size\u001B[38;5;241m=\u001B[39mspatialSize),\n\u001B[1;32m     33\u001B[0m ])\u001B[38;5;241m.\u001B[39mcuda()\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Compose' object has no attribute 'cuda'"
     ]
    }
   ],
   "source": [
    "# 3. Data and Training. Put all the data-augmentation transforms here\n",
    "#train_transforms = monai.transforms.Compose([\n",
    "#    FILLTHIS\n",
    "#])\n",
    "\n",
    "#val_transforms = monai.transforms.Compose([\n",
    "#    FILLTHIS\n",
    "#])\n",
    "\n",
    "newVoxelDims = np.median(voxelDims, axis=0)\n",
    "spatialSize = 320, 320, 32\n",
    "\n",
    "train_transforms = monai.transforms.Compose([\n",
    "    monai.transforms.LoadImaged(keys=['image', 'label']),\n",
    "    monai.transforms.EnsureChannelFirstd(keys=['image', 'label']),\n",
    "    monai.transforms.Spacingd(keys=['image', 'label'], pixdim=newVoxelDims, mode=[\"bilinear\", \"nearest\"]),\n",
    "    monai.transforms.ResizeWithPadOrCropd(keys=['image', 'label'], spatial_size=spatialSize),\n",
    "    #monai.transforms.RandSpatialCropd(keys=['image', 'label'], roi_size=[-1, -1, 1]),\n",
    "    #monai.transforms.SqueezeDimd(keys=['image', 'label'], dim=-1),\n",
    "    ## TODO: Add more Data Augmentation functions here\n",
    "    #monai.transforms.RandRotated(keys=['image', 'label'], range_z = 180, prob = 0.5, mode='nearest'),\n",
    "    #monai.transforms.RandGaussianSmoothd(keys=['image'], sigma_x=(0.25, 1.5), sigma_y=(0.25, 1.5), sigma_z=(0.25, 1.5), approx='erf', prob=0.5),\n",
    "    #monai.transforms.RandGaussianNoised(keys=['image'], prob=0.5, mean=0.0, std=0.5),\n",
    "\n",
    "    monai.transforms.AsDiscreted(keys=['label'], to_onehot=4) # Convert \"label\" to onehot encoded.\n",
    "])\n",
    "\n",
    "val_transforms = monai.transforms.Compose([\n",
    "    monai.transforms.LoadImaged(keys=['image', 'label']),\n",
    "    monai.transforms.EnsureChannelFirstd(keys=['image', 'label']),\n",
    "    monai.transforms.Spacingd(keys=['image', 'label'], pixdim=newVoxelDims, mode=[\"bilinear\", \"nearest\"]),\n",
    "    monai.transforms.ResizeWithPadOrCropd(keys=['image', 'label'], spatial_size=spatialSize),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6e604723-4ae5-42d7-a077-2a75d3abdbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Your dataloader gives you 3D images\n",
      "✅ Your images have the same number of slices.\n"
     ]
    }
   ],
   "source": [
    "# Verifying that you're training on 3D images\n",
    "dummy_dataset = Dataset(data=train_files, transform=train_transforms)\n",
    "dummy_loader = DataLoader(dummy_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "err1, err2 = False, False\n",
    "num_slices = set()\n",
    "for dummy_data in dummy_loader:\n",
    "    if len(dummy_data['image'].shape) != 5:\n",
    "        err1 = True\n",
    "        \n",
    "    num_slices.add(dummy_data['image'].shape[-1])\n",
    "    if len(num_slices) != 1:\n",
    "        err2 = True\n",
    "\n",
    "if err1:\n",
    "    print(\"❌ Your dataloader doesn't give you 3D images.\")\n",
    "else:\n",
    "    print(\"✅ Your dataloader gives you 3D images\")\n",
    "\n",
    "if err2:\n",
    "    print(\"❌ Your images have different number of slices, so, they won't fit into a single mini-batch. Maybe you need to use ResizeWithPadOrCropd?\")\n",
    "else:\n",
    "    print(\"✅ Your images have the same number of slices.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e7f195-06b2-4792-b742-f40d6b7cd95a",
   "metadata": {},
   "source": [
    "## 1.4 Verifying your model\n",
    "\n",
    "At this point, you should have:\n",
    "* [**Data**] The dataset split into train-validation-set.\n",
    "* [**Data,Training**] The data-augmentation transforms, including the \"preprocessing\" transforms.\n",
    "* [**Training**] The data loaders.\n",
    "* [**Model**] A 3D model to train.\n",
    "\n",
    "Let's try to do inference in a few images. Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a42213c7-2a94-4534-9472-c88881f269cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 320, 320, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[66], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m inputs \u001B[38;5;241m=\u001B[39m dummy_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(inputs\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m----> 5\u001B[0m pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m5\u001B[39m:\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/Works/Dtu/deep-learning-experimental-3D-image/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Works/Dtu/deep-learning-experimental-3D-image/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Works/Dtu/deep-learning-experimental-3D-image/venv/lib/python3.10/site-packages/monai/networks/nets/dynunet.py:269\u001B[0m, in \u001B[0;36mDynUNet.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    268\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m--> 269\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mskip_layers\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    270\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_block(out)\n\u001B[1;32m    271\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdeep_supervision:\n",
      "File \u001B[0;32m~/Works/Dtu/deep-learning-experimental-3D-image/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Works/Dtu/deep-learning-experimental-3D-image/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Works/Dtu/deep-learning-experimental-3D-image/venv/lib/python3.10/site-packages/monai/networks/nets/dynunet.py:47\u001B[0m, in \u001B[0;36mDynUNetSkipLayer.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 47\u001B[0m     downout \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownsample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m     nextout \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnext_layer(downout)\n\u001B[1;32m     49\u001B[0m     upout \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupsample(nextout, downout)\n",
      "File \u001B[0;32m~/Works/Dtu/deep-learning-experimental-3D-image/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Works/Dtu/deep-learning-experimental-3D-image/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Works/Dtu/deep-learning-experimental-3D-image/venv/lib/python3.10/site-packages/monai/networks/blocks/dynunet_block.py:171\u001B[0m, in \u001B[0;36mUnetBasicBlock.forward\u001B[0;34m(self, inp)\u001B[0m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, inp):\n\u001B[0;32m--> 171\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43minp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    172\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(out)\n\u001B[1;32m    173\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlrelu(out)\n",
      "File \u001B[0;32m~/Works/Dtu/deep-learning-experimental-3D-image/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Works/Dtu/deep-learning-experimental-3D-image/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Works/Dtu/deep-learning-experimental-3D-image/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/Works/Dtu/deep-learning-experimental-3D-image/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Works/Dtu/deep-learning-experimental-3D-image/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Works/Dtu/deep-learning-experimental-3D-image/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:610\u001B[0m, in \u001B[0;36mConv3d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    609\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 610\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Works/Dtu/deep-learning-experimental-3D-image/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:605\u001B[0m, in \u001B[0;36mConv3d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    593\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    594\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv3d(\n\u001B[1;32m    595\u001B[0m         F\u001B[38;5;241m.\u001B[39mpad(\n\u001B[1;32m    596\u001B[0m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    603\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups,\n\u001B[1;32m    604\u001B[0m     )\n\u001B[0;32m--> 605\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv3d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    606\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\n\u001B[1;32m    607\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Works/Dtu/deep-learning-experimental-3D-image/venv/lib/python3.10/site-packages/monai/data/meta_tensor.py:282\u001B[0m, in \u001B[0;36mMetaTensor.__torch_function__\u001B[0;34m(cls, func, types, args, kwargs)\u001B[0m\n\u001B[1;32m    280\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    281\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m--> 282\u001B[0m ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__torch_function__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtypes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;66;03m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001B[39;00m\n\u001B[1;32m    284\u001B[0m \u001B[38;5;66;03m# if \"out\" in kwargs:\u001B[39;00m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;66;03m#     return ret\u001B[39;00m\n\u001B[1;32m    286\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _not_requiring_metadata(ret):\n",
      "File \u001B[0;32m~/Works/Dtu/deep-learning-experimental-3D-image/venv/lib/python3.10/site-packages/torch/_tensor.py:1418\u001B[0m, in \u001B[0;36mTensor.__torch_function__\u001B[0;34m(cls, func, types, args, kwargs)\u001B[0m\n\u001B[1;32m   1415\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m\n\u001B[1;32m   1417\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _C\u001B[38;5;241m.\u001B[39mDisableTorchFunctionSubclass():\n\u001B[0;32m-> 1418\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1419\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m func \u001B[38;5;129;01min\u001B[39;00m get_default_nowrap_functions():\n\u001B[1;32m   1420\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m ret\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "# 2. Model. Now, use your model to do inference in a few images\n",
    "for i, dummy_data in enumerate(dummy_loader):\n",
    "    inputs = dummy_data['image']\n",
    "    print(inputs.shape)\n",
    "    pred = model(inputs)\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942886be-2a3a-4931-adf3-ac189c892b4d",
   "metadata": {},
   "source": [
    "If you used the same architecture of the model that you trained last week, **you got probably an error** (if everything went fine, congrats!).\n",
    "\n",
    "This error is likely caused due to the small and incompatible number of features in the last dimension. Remember from last week that it is important to find an appropriate image size to train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b73678-cc19-4c5a-8b72-8b361736e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367fa77b-3e05-4ab1-9068-3571511d29c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will work\n",
    "inputs = torch.rand((1, 1, 128, 128, 32)).cuda()\n",
    "pred = model(inputs).cuda()\n",
    "print(pred.shape)\n",
    "\n",
    "# This will not work\n",
    "# inputs = torch.rand((1, 1, 320, 320, 11))\n",
    "# pred = model(inputs)\n",
    "# print(pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46431626-c556-4710-ae8c-41c5ff9050a4",
   "metadata": {},
   "source": [
    "Remember that the encoder of UNet-like architectures downsamples the input images multiple times. If, e.g., our feature maps have a size of 320x320x11, after downsampling them (with strides=2) once, the feature maps will be 160x160xF. What would F be: Five or Six? It depends on the configuration of the network. This detail might not be too important in the encoder, but it is an important consideration in the decoder when trying to concatenate the feature maps from the encoder and decoder. Chances are that your code raised a RuntimeError when attempting to torch.cat in the skip layers.\n",
    "\n",
    "[<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" width=\"700px\">](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)\n",
    "\n",
    "> ---\n",
    "> **Task:**\n",
    "> \n",
    "> Find:\n",
    "> 1. A network configuration,\n",
    "> 2. An image size (for the ResizeWithPadOrCropd transform) and\n",
    "> 3. A batch size\n",
    "> \n",
    "> that are **compatible**, and that **fit** in the GPU.\n",
    ">\n",
    "> **Note**: Regarding the image size for ResizeWithPadOrCrop, last week we only cared about finding an appropriate HxW and did not care about the Depth dimension because we were training on slices. Now, you must find HxWxD.\n",
    "> \n",
    "> ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8e7a87-0871-4a8a-a29b-77f242377719",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 320\n",
    "inferer = monai.inferers.SlidingWindowInferer(roi_size=(a, a,-1), sw_batch_size=1)\n",
    "inferer(torch.randn(1,1,a,a,32).cuda(), model).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca36011-17fd-466c-8f89-d3d44c25ea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = 320\n",
    "for a in [100 + 2*i for i in range(400)]:\n",
    "    try:\n",
    "        inferer = monai.inferers.SliceInferer(roi_size=(a, a), sw_batch_size=1, spatial_dim=2)\n",
    "        inferer(torch.randn(1,1,a,a,28).cuda(), model).shape\n",
    "        print(a)\n",
    "    except:\n",
    "        pass\n",
    "    # print(\"e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7987a4-1ec1-4b9e-ab7d-995c4b8b451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters (next three lines) #\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 1\n",
    "EVAL_EVERY = 1\n",
    "\n",
    "train_dataset = Dataset(data=train_files, transform=train_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataset = Dataset(data=val_files, transform=val_transforms)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# More design decisions (model, loss, optimizer) #\n",
    "\n",
    "loss_fn = monai.losses.DiceLoss(softmax=True, to_onehot_y=False) # Apply \"softmax\" to the output of the network and don't convert to onehot because this is done already by the transforms.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) \n",
    "\n",
    "# inferer = monai.inferers.SliceInferer(roi_size=[320, 320], cval=-1, sw_batch_size=1)\n",
    "# inferer = monai.inferers.SimpleInferer()#monai.inferers.SliceInferer(roi_size=(320, 320), sw_batch_size=1, cval=-1, progress=True)\n",
    "inferer = monai.inferers.SlidingWindowInferer(roi_size=(320, 320, -1), sw_batch_size=1)\n",
    "\n",
    "\n",
    "all_losses = []\n",
    "all_dices = []\n",
    "all_hd95 = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for tr_data in train_loader:\n",
    "        inputs = tr_data['image'].cuda()\n",
    "        targets = tr_data['label'].cuda()\n",
    "\n",
    "        # Forward -> Backward -> Step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.detach()\n",
    "        step += 1\n",
    "        \n",
    "    # Log and store average epoch loss\n",
    "    epoch_loss = epoch_loss.item() / step\n",
    "    all_losses.append(epoch_loss)\n",
    "    print(f'Mean loss: {epoch_loss}')\n",
    "\n",
    "    if epoch % EVAL_EVERY == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():  # Do not need gradients for this part\n",
    "            for val_data in val_loader:\n",
    "                inputs = val_data['image'].cuda()\n",
    "                targets = val_data['label'].cuda()\n",
    "\n",
    "                # Gets the prediction in the entire 3D image. Shape = BCHWD\n",
    "                # inputs = inputs.permute(0, 1, 4, 2, 3) # changed                \n",
    "                prediction = inferer(inputs=inputs, network=model)\n",
    "\n",
    "                # Inverses the transformations, i.e., it:\n",
    "                # - Unpads/Uncrops the image\n",
    "                # - Resamples back to the original voxel resolution\n",
    "                batch_inverter = BatchInverseTransform(val_transforms, val_loader)\n",
    "                with allow_missing_keys_mode(val_transforms):\n",
    "                    inversed_prediction = batch_inverter({\"label\": prediction}) # Shape: inversed_prediction[B]['label'].shape = CHWD\n",
    "                    inversed_targets = batch_inverter({\"label\": targets}) # Shape: inversed_targets[B]['label'].shape = 1HWD\n",
    "\n",
    "                # Applying argmax on that prediction\n",
    "                inversed_prediction = [monai.transforms.AsDiscrete(argmax=True)(pred['label']) for pred in inversed_prediction]\n",
    "\n",
    "                val_dices = []\n",
    "                val_hd95 = []\n",
    "                # Compute the dice coefficients\n",
    "                for b in range(prediction.shape[0]):\n",
    "                    dices_tmp = []\n",
    "                    hd95_tmp = []\n",
    "                    for c in range(1, prediction.shape[1]): # Iterate over each class (except the first one, that corresponds to the background)\n",
    "                        dices_tmp.append( metric.dc( 1*(inversed_prediction[b].cpu().detach().numpy()==c),\n",
    "                                                   1*(inversed_targets[b]['label'].cpu().detach().numpy()==c) ) )\n",
    "                        # hd95_tmp.append(metric.hd95(prediction[b][c], targets[b][c], voxelspacing=newVoxelDims))\n",
    "                    val_dices.append(dices_tmp)\n",
    "                    val_hd95.append(metric.hd95(prediction.argmax(dim=1).squeeze(), targets.squeeze(), voxelspacing=newVoxelDims))\n",
    "\n",
    "        # Log and store the average Dice coefficient across images; one value per class\n",
    "        all_dices.append(np.mean(val_dices, axis=0))\n",
    "        all_hd95.append(np.mean(val_hd95, axis=0))\n",
    "        print(f'Dice coefficients: {all_dices[-1]}')\n",
    "        print(f'HD distances: {all_hd95[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4acc00-2c9a-4f01-8d79-1c15a5392c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19a22ac5-0e68-4c13-8a71-90d298a3c62c",
   "metadata": {},
   "source": [
    "> ---\n",
    "> **Task:**\n",
    "> \n",
    "> Similarly to last week, visualize 1) the training loss, 2) the validation dice coefficient, and 3) the validation of the other metric of your choice.\n",
    ">\n",
    "> ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "da89b816-234a-48b1-b3f8-448939dc8981",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[68], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mprediction\u001B[49m\u001B[38;5;241m.\u001B[39mshape, prediction\u001B[38;5;241m.\u001B[39margmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39msqueeze()\u001B[38;5;241m.\u001B[39mshape, targets\u001B[38;5;241m.\u001B[39mshape, targets\u001B[38;5;241m.\u001B[39msqueeze()\u001B[38;5;241m.\u001B[39mshape)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'prediction' is not defined"
     ]
    }
   ],
   "source": [
    "print(prediction.shape, prediction.argmax(dim=1).squeeze().shape, targets.shape, targets.squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e9a6f17d-0e58-4e77-91d9-cffe3fd81f16",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[69], line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m axs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mset_ylabel(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     12\u001B[0m axs[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mset_title(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation DICE coefficients\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 13\u001B[0m axs[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mplot(\u001B[43mad\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m, label \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLeft Ventricle\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     14\u001B[0m axs[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mplot(ad[:,\u001B[38;5;241m1\u001B[39m], label \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMyocardium\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     15\u001B[0m axs[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mplot(ad[:,\u001B[38;5;241m2\u001B[39m], label \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRight Ventricle\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mIndexError\u001B[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code for the task here\n",
    "# plt.plot(all_losses)\n",
    "ad = np.array(all_dices)\n",
    "ah = np.array(all_hd95)\n",
    "\n",
    "fig, axs = plt.subplots(1,3, figsize = (12,5))\n",
    "axs[0].set_title(\"Training loss\")\n",
    "axs[0].plot(all_losses)\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[0].set_ylabel(\"loss\")\n",
    "\n",
    "axs[1].set_title(\"Validation DICE coefficients\")\n",
    "axs[1].plot(ad[:,0], label = \"Left Ventricle\")\n",
    "axs[1].plot(ad[:,1], label = \"Myocardium\")\n",
    "axs[1].plot(ad[:,2], label = \"Right Ventricle\")\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].set_ylabel(\"DICE coefficient\")\n",
    "plt.legend()\n",
    "\n",
    "axs[2].set_title(\"Validation 95th percentile of the Hausdorff Distance\")\n",
    "axs[2].plot(ah)#, label = \"Left Ventricle\")\n",
    "# axs[2].plot(ah[:,1], label = \"Myocardium\")\n",
    "# axs[2].plot(ah[:,2], label = \"Right Ventricle\")\n",
    "axs[2].set_xlabel(\"Epoch\")\n",
    "axs[2].set_ylabel(\"Distance\")\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6008ce-07e5-4c2f-9347-53ea443c6058",
   "metadata": {},
   "source": [
    "## 1.6 Inference in the test set\n",
    "\n",
    "> ---\n",
    "> **Task**: Compute the predictions of the test set images.\n",
    ">\n",
    "> **Important remark**: Fill the list \"test_dices\", which is a list of lists. E.g., test_dices = [ [id1, dice1, dice2, dice3], [id2, dice1, dice2, dice3], ...]. Here, idN is the name of the file (e.g., patient019_frame12) that you get from `test_data['id']`, and each dice corresponds to the Dice coefficient in each class except in 'class 0' (background).\n",
    ">\n",
    "> ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "14b626d6-1bc1-4d18-b431-ff8f6cc79de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6e84778f-1227-4dc3-995a-749802260f4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testFiles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[71], line 12\u001B[0m\n\u001B[1;32m      3\u001B[0m test_transforms \u001B[38;5;241m=\u001B[39m monai\u001B[38;5;241m.\u001B[39mtransforms\u001B[38;5;241m.\u001B[39mCompose([\n\u001B[1;32m      4\u001B[0m     monai\u001B[38;5;241m.\u001B[39mtransforms\u001B[38;5;241m.\u001B[39mLoadImaged(keys\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m]),\n\u001B[1;32m      5\u001B[0m     monai\u001B[38;5;241m.\u001B[39mtransforms\u001B[38;5;241m.\u001B[39mEnsureChannelFirstd(keys\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m]),\n\u001B[1;32m      6\u001B[0m     monai\u001B[38;5;241m.\u001B[39mtransforms\u001B[38;5;241m.\u001B[39mSpacingd(keys\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m], pixdim\u001B[38;5;241m=\u001B[39mnewVoxelDims, mode\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbilinear\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnearest\u001B[39m\u001B[38;5;124m\"\u001B[39m]),\n\u001B[1;32m      7\u001B[0m     monai\u001B[38;5;241m.\u001B[39mtransforms\u001B[38;5;241m.\u001B[39mResizeWithPadOrCropd(keys\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m], spatial_size\u001B[38;5;241m=\u001B[39mspatialSize),\n\u001B[1;32m      8\u001B[0m ])\n\u001B[1;32m     10\u001B[0m inferer \u001B[38;5;241m=\u001B[39m monai\u001B[38;5;241m.\u001B[39minferers\u001B[38;5;241m.\u001B[39mSlidingWindowInferer(roi_size\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m320\u001B[39m, \u001B[38;5;241m320\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), sw_batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 12\u001B[0m test_dataset \u001B[38;5;241m=\u001B[39m Dataset(data\u001B[38;5;241m=\u001B[39m\u001B[43mtestFiles\u001B[49m, transform\u001B[38;5;241m=\u001B[39mtest_transforms)\n\u001B[1;32m     13\u001B[0m test_loader \u001B[38;5;241m=\u001B[39m DataLoader(test_dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     16\u001B[0m model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mcpu()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'testFiles' is not defined"
     ]
    }
   ],
   "source": [
    "test_dices = []\n",
    "\n",
    "test_transforms = monai.transforms.Compose([\n",
    "    monai.transforms.LoadImaged(keys=['image', 'label']),\n",
    "    monai.transforms.EnsureChannelFirstd(keys=['image', 'label']),\n",
    "    monai.transforms.Spacingd(keys=['image', 'label'], pixdim=newVoxelDims, mode=[\"bilinear\", \"nearest\"]),\n",
    "    monai.transforms.ResizeWithPadOrCropd(keys=['image', 'label'], spatial_size=spatialSize),\n",
    "])\n",
    "\n",
    "inferer = monai.inferers.SlidingWindowInferer(roi_size=(320, 320, -1), sw_batch_size=1)\n",
    "\n",
    "test_dataset = Dataset(data=test_files, transform=test_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "model = model.cpu()\n",
    "test_dices = []\n",
    "for test_data in tqdm(test_loader):\n",
    "    inputs = test_data['image']#.cuda()\n",
    "    targets = test_data['label']#.cuda()\n",
    "    ids = test_data['id']\n",
    "\n",
    "    prediction = inferer(inputs=inputs, network=model)\n",
    "    \n",
    "    batch_inverter = BatchInverseTransform(test_transforms, test_loader)\n",
    "    with allow_missing_keys_mode(test_transforms):\n",
    "        inversed_prediction = batch_inverter({\"label\": prediction}) # Shape: inversed_prediction[B]['label'].shape = CHWD\n",
    "        inversed_targets = batch_inverter({\"label\": targets}) # Shape: inversed_targets[B]['label'].shape = 1HWD\n",
    "\n",
    "    # Applying argmax on that prediction\n",
    "    inversed_prediction = [monai.transforms.AsDiscrete(argmax=True)(pred['label']) for pred in inversed_prediction]\n",
    "\n",
    "    # Compute the dice coefficients\n",
    "    for b in range(prediction.shape[0]):\n",
    "        print(\" --- \", ids)\n",
    "        dices = [ids[0]]\n",
    "        for c in range(1, prediction.shape[1]): # Iterate over each class (except the first one, that corresponds to the background)\n",
    "            dice = metric.dc( 1*(inversed_prediction[b].cpu().detach().numpy()==c), 1*(inversed_targets[b]['label'].cpu().detach().numpy()==c))\n",
    "            dices.append(dice)\n",
    "        print(dices)\n",
    "        test_dices.append(dices)\n",
    "\n",
    "# Checking that your \"test_dices\" has the right format.\n",
    "assert len(test_dices) == 40\n",
    "for line in test_dices:\n",
    "    assert len(line) == 4\n",
    "\n",
    "for frameid, dice1, dice2, dice3 in test_dices:\n",
    "    print(f\"Frame ID: {frameid}. Dice coeff. (Class 1): {dice1}. Dice coeff. (Class 2): {dice2}. Dice coeff. (Class 3): {dice3}\")\n",
    "\n",
    "avg_dice = np.mean([[d1, d2, d3] for _, d1, d2, d3 in test_dices])\n",
    "print(f\"Final average dice: {avg_dice}\") # This should be one single number. It will be later used to compare which model is better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af59f712-0912-4e06-9f9b-b54d1f2d6dd9",
   "metadata": {},
   "source": [
    "## 1.7 Misc: Saving and loading PyTorch models\n",
    "\n",
    "In Pytorch we are typically interested in saving the trained models to, later, use it for inference. Here, we say \"saving models\" in a loose way since, in practice, we don't save the model; we save the **parameters** of the model. Saving these parameters can be done in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "39ac284b-1b14-420a-9a90-6ed958c6efbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'my_model'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[72], line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Note: as explained above, if you try to load your saved weights into this model, you'll get an error.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# This is because the models have different architecture.\u001B[39;00m\n\u001B[1;32m      3\u001B[0m model2 \u001B[38;5;241m=\u001B[39m DynUNet(spatial_dims\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, in_channels\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, out_channels\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m,\n\u001B[1;32m      4\u001B[0m                 kernel_size\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m5\u001B[39m],\n\u001B[1;32m      5\u001B[0m                 strides\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m2\u001B[39m], upsample_kernel_size\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m2\u001B[39m])\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[0;32m----> 6\u001B[0m model2\u001B[38;5;241m.\u001B[39mload_state_dict(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmy_model\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/Works/Dtu/deep-learning-experimental-3D-image/venv/lib/python3.10/site-packages/torch/serialization.py:998\u001B[0m, in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[1;32m    995\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    996\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m--> 998\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[1;32m    999\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[1;32m   1000\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[1;32m   1001\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[1;32m   1002\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[1;32m   1003\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[0;32m~/Works/Dtu/deep-learning-experimental-3D-image/venv/lib/python3.10/site-packages/torch/serialization.py:445\u001B[0m, in \u001B[0;36m_open_file_like\u001B[0;34m(name_or_buffer, mode)\u001B[0m\n\u001B[1;32m    443\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[1;32m    444\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[0;32m--> 445\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    446\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    447\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[0;32m~/Works/Dtu/deep-learning-experimental-3D-image/venv/lib/python3.10/site-packages/torch/serialization.py:426\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[0;34m(self, name, mode)\u001B[0m\n\u001B[1;32m    425\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[0;32m--> 426\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'my_model'"
     ]
    }
   ],
   "source": [
    "# Note: as explained above, if you try to load your saved weights into this model, you'll get an error.\n",
    "# This is because the models have different architecture.\n",
    "model2 = DynUNet(spatial_dims=3, in_channels=1, out_channels=4,\n",
    "                kernel_size=[5, 5, 5, 5, 5],\n",
    "                strides=[1, 2, 2, 2, 2], upsample_kernel_size=[2, 2, 2, 2]).cuda()\n",
    "model2.load_state_dict(torch.load(\"my_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e7f219-761c-499c-9000-f280a1195f51",
   "metadata": {},
   "source": [
    "## 1.8 Misc: Saving segmentations (numpy arrays) as Nifti files (.nii/.nii.gz)\n",
    "\n",
    "After generating automatic segmentations, it is usually the case that we want to save them in files to later visualize them or quantify certain regions of interest. Saving segmentations as Nifti files is in practice very easy, however, it is **extremely important** to be mindful about the images' header. These headers contain important information, as you saw last week, about the voxel resolution, but also about the way the images should be displayed when opened in programs such as ITKsnap. A safe way to save segmentation as nifti files is to, first, open the image they correspond to, and, afterwards, using that images' header and affine transform. See example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a8db5ac7-37e1-423a-95cc-094dc6e552e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IMAGEPATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[73], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Open the original image\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m image_original \u001B[38;5;241m=\u001B[39m nib\u001B[38;5;241m.\u001B[39mload(\u001B[43mIMAGEPATH\u001B[49m)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Suppose that the segmentation provided by your model is a numpy array\u001B[39;00m\n\u001B[1;32m      4\u001B[0m segmentation_arr \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandom((\u001B[38;5;241m255\u001B[39m, \u001B[38;5;241m255\u001B[39m, \u001B[38;5;241m255\u001B[39m))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'IMAGEPATH' is not defined"
     ]
    }
   ],
   "source": [
    "# Open the original image\n",
    "image_original = nib.load(IMAGEPATH)\n",
    "# Suppose that the segmentation provided by your model is a numpy array\n",
    "segmentation_arr = np.random.random((255, 255, 255))\n",
    "# Create a Nifti image with nibabel\n",
    "segmentation_im = nib.Nifti1Image(segmentation_arr, affine=image_original.affine, header=image_original.header)\n",
    "# Save the image\n",
    "nib.save(segmentation_im, OUTPUTPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ef5d31-04a6-4f89-9a0f-4a211ff6978a",
   "metadata": {},
   "source": [
    "# Part 2: Logging, batch jobs, and quantification\n",
    "\n",
    "In this part, you will learn how to monitor the training of deep learning models with Weights and Biases (wandb.ai), and how to submit jobs to a non-interactive non-shared GPU at DTU's main cluster. Afterwards, you will train various models, select the highest-performing model, segment the images in the test set, and quantify the volume of the segmented regions of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50646228-2a8f-4d00-8059-92d2cb42338d",
   "metadata": {},
   "source": [
    "## 2.1 Logging with Weights and Biases\n",
    "\n",
    "Afyer you create a free account in wandb.ai, you will need to run this code (from the tutorial) and paste the API key as shown in the slides.\n",
    "\n",
    "\n",
    "> ---\n",
    "> **Tasks:**\n",
    "> * Create an account in wandb.ai\n",
    "> * Run this code, paste the API key.\n",
    "> * Visualize the information sent to wandb.ai\n",
    "> \n",
    "> ---\n",
    "> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0530a4e4-11ae-48d7-ae92-f28c8f56d048",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "73458e7d-bf6a-458e-9c3c-1db861993d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mkejaj-777\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kazi/Works/Dtu/deep-learning-experimental-3D-image/Week-4/wandb/run-20240417_225735-tkjm4wwd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kejaj-777/my-awesome-project/runs/tkjm4wwd' target=\"_blank\">fallen-voice-1</a></strong> to <a href='https://wandb.ai/kejaj-777/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kejaj-777/my-awesome-project' target=\"_blank\">https://wandb.ai/kejaj-777/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kejaj-777/my-awesome-project/runs/tkjm4wwd' target=\"_blank\">https://wandb.ai/kejaj-777/my-awesome-project/runs/tkjm4wwd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.008 MB of 0.011 MB uploaded\\r'), FloatProgress(value=0.6818603083663325, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▁▄▇█▆▆▇</td></tr><tr><td>loss</td><td>█▃▃▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75309</td></tr><tr><td>loss</td><td>0.23488</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fallen-voice-1</strong> at: <a href='https://wandb.ai/kejaj-777/my-awesome-project/runs/tkjm4wwd' target=\"_blank\">https://wandb.ai/kejaj-777/my-awesome-project/runs/tkjm4wwd</a><br/> View project at: <a href='https://wandb.ai/kejaj-777/my-awesome-project' target=\"_blank\">https://wandb.ai/kejaj-777/my-awesome-project</a><br/>Synced 4 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240417_225735-tkjm4wwd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "# simulate training\n",
    "epochs = 10\n",
    "offset = random.random() / 5\n",
    "for epoch in range(2, epochs):\n",
    "    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "    loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "    \n",
    "    # log metrics to wandb\n",
    "    wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "    \n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1189a465-73db-4c96-9057-d713b1daba07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This won't be logged\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kazi/Works/Dtu/deep-learning-experimental-3D-image/Week-4/wandb/run-20240417_225743-06fwsr9d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kejaj-777/my-awesome-project/runs/06fwsr9d' target=\"_blank\">vital-field-2</a></strong> to <a href='https://wandb.ai/kejaj-777/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kejaj-777/my-awesome-project' target=\"_blank\">https://wandb.ai/kejaj-777/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kejaj-777/my-awesome-project/runs/06fwsr9d' target=\"_blank\">https://wandb.ai/kejaj-777/my-awesome-project/runs/06fwsr9d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will be logged\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 28.7%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>training_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>training_loss</td><td>0.09702</td></tr><tr><td>validation_loss</td><td>0.09573</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vital-field-2</strong> at: <a href='https://wandb.ai/kejaj-777/my-awesome-project/runs/06fwsr9d' target=\"_blank\">https://wandb.ai/kejaj-777/my-awesome-project/runs/06fwsr9d</a><br/> View project at: <a href='https://wandb.ai/kejaj-777/my-awesome-project' target=\"_blank\">https://wandb.ai/kejaj-777/my-awesome-project</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240417_225743-06fwsr9d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "print(\"This won't be logged\")\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"This will be logged\")\n",
    "\n",
    "# simulate training\n",
    "epochs = 100\n",
    "val_iter = 5\n",
    "offset = random.random() / 5\n",
    "for epoch in range(1, epochs+1):\n",
    "    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "    loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "    logthis = {\"training_loss\": loss}\n",
    "\n",
    "    if epoch % val_iter == 0:\n",
    "        val_loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "        logthis[\"validation_loss\"] = val_loss\n",
    "    \n",
    "    # log metrics to wandb\n",
    "    wandb.log(logthis)\n",
    "    \n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c740ded-cdff-4704-aa68-d214ff2670fc",
   "metadata": {},
   "source": [
    "#### You can also send \"images\" to WANDB, which can be useful to assess visually, e.g., how well the model is learning to segment images. But be mindful about what you send. Your data might have a license that won't allow to share it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4f117b8b-e500-486e-9a66-6d863a17b5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kazi/Works/Dtu/deep-learning-experimental-3D-image/Week-4/wandb/run-20240417_225808-niryiha2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kejaj-777/my-awesome-project/runs/niryiha2' target=\"_blank\">deft-spaceship-3</a></strong> to <a href='https://wandb.ai/kejaj-777/my-awesome-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kejaj-777/my-awesome-project' target=\"_blank\">https://wandb.ai/kejaj-777/my-awesome-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kejaj-777/my-awesome-project/runs/niryiha2' target=\"_blank\">https://wandb.ai/kejaj-777/my-awesome-project/runs/niryiha2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "for i in range(10):\n",
    "    im_arr = np.random.random((100, 100))\n",
    "    images = wandb.Image(im_arr, caption=\"Top: Output, Bottom: Input\")\n",
    "    wandb.log({\"examples\": images})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612059e2-ac4d-458b-9b0f-d34f49078f08",
   "metadata": {},
   "source": [
    "> ---\n",
    "> **Task:**\n",
    "> \n",
    "> Add logging to the code you used to train your model. Before, we were saving the validation loss and validation dice in a list. Now, in addition to that, send it to W&B. After this, verify on wandb.ai website that you are actually saving what you want to save. Remember to send the other metric that you're measuring. In total, you should send, at least, 7 values: training loss, dice coefficient of classes 1-3, and the other metric's measurements of classes 1-3.\n",
    ">\n",
    "> ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc9cd75-15d5-46d5-8dee-89fb7f4f495b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
